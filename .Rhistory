state = pd.read_csv(STATE_CSV)
import pandas as pd
from pathlib import Path
import pandas as pd
import numpy as np
from scipy.stats import trim_mean
from scipy.stats import trim_mean
from statsmodels import robust
import wquantiles
import seaborn as sns
import seaborn as sns
import matplotlib.pylab as plt
try:
import common
DATA = common.dataDirectory()
except ImportError:
DATA = Path().resolve() / 'data'
STATE_CSV = DATA / 'state.csv'
state = pd.read_csv(STATE_CSV)
print(state.head(8))
# Compute the mean, trimmed mean, and median for Population. For `mean` and `median` we can use the _pandas_ methods of the data frame. The trimmed mean requires the `trim_mean` function in _scipy.stats_.
state = pd.read_csv(STATE_CSV)
state.__class__
print(state['Population'].mean())
print(trim_mean(state['Population'], 0.1))
print(state['Population'].median())
print(state['Murder.Rate'].mean())
print(np.average(state['Murder.Rate'], weights=state['Population']))
print(wquantiles.median(state['Murder.Rate'], weights=state['Population']))
print(state.head(8))
print(state['Population'].std())
print(state['Population'].quantile(0.75) - state['Population'].quantile(0.25))
print(robust.scale.mad(state['Population']))
print(abs(state['Population'] - state['Population'].median()).median() / 0.6744897501960817)
abs(state['Population'] - state['Population'].median()).median()
print(robust.scale.mad(state['Population']))
print(abs(state['Population'] - state['Population'].median()).median() / 0.6744897501960817)
print(state['Population'].mean())
print(state['Population'].median())
4436369.5/6162876.3
print(state['Murder.Rate'].quantile([0.05, 0.25, 0.5, 0.75, 0.95]))
ax = (state['Population']/1_000_000).plot.box(figsize=(3, 4))
ax.set_ylabel('Population (millions)')
plt.tight_layout()
plt.show()
ax = (state['Population']/1_000_000).plot.box(figsize=(3, 4))
ax.set_ylabel('Population (millions)')
# _Pandas_ has the `quantile` method for data frames.
print(state['Murder.Rate'].quantile([0.05, 0.25, 0.5, 0.75, 0.95]))
# _Pandas_ provides a number of basic exploratory plots; one of them are boxplots
ax = (state['Population']/1_000_000).plot.box(figsize=(3, 4))
ax.set_ylabel('Population (millions)')
plt.tight_layout()
plt.show()
ax.show()
plt.tight_layout()
plt.show()
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 10) #cut into 10 bins
print(binnedPopulation.value_counts())
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 11) #cut into 10 bins
print(binnedPopulation.value_counts())
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 10) #cut into 10 bins
print(binnedPopulation.value_counts())
state["Population"].max()
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 10) #cut into 10 bins
state["Population"].max()
print(binnedPopulation.value_counts())
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 11) #cut into 10 bins
state["Population"].max()
print(binnedPopulation.value_counts())
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 20) #cut into 10 bins
state["Population"].max()
print(binnedPopulation.value_counts())
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 50) #cut into 10 bins
state["Population"].max()
print(binnedPopulation.value_counts())
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 10) #cut into 10 bins
state["Population"].max()
print(binnedPopulation.value_counts())
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 10) #cut into
print(binnedPopulation.value_counts())
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 10) #cut into
print(binnedPopulation.value_counts())
binnedPopulation.name = 'binnedPopulation'
df = pd.concat([state, binnedPopulation], axis=1)
df = df.sort_values(by='Population')
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 10) #cut into
print(binnedPopulation.value_counts())
binnedPopulation.name = 'binnedPopulation'
df = pd.concat([state, binnedPopulation], axis=1)
df = df.sort_values(by='Population')
groups = []
for group, subset in df.groupby(by='binnedPopulation'):
groups.append({
'BinRange': group,
'Count': len(subset),
'States': ','.join(subset.Abbreviation)
})
print(pd.DataFrame(groups))
ax = (state['Population'] / 1_000_000).plot.hist(figsize=(4, 4))
ax.set_xlabel('Population (millions)')
plt.tight_layout()
plt.show()
binnedPopulation = pd.cut(state['Population'], 10) #cut into
reticulate::repl_python()
print(binnedPopulation.value_counts())
binnedPopulation.name = 'binnedPopulation'
df = pd.concat([state, binnedPopulation], axis=1)
df = df.sort_values(by='Population')
groups = []
for group, subset in df.groupby(by='binnedPopulation'):
groups.append({
'BinRange': group,
'Count': len(subset),
'States': ','.join(subset.Abbreviation)
})
print(pd.DataFrame(groups))
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 10) #cut into 10 bins
print(binnedPopulation.value_counts())
binnedPopulation.name = 'binnedPopulation'
df = pd.concat([state, binnedPopulation], axis=1)
df = df.sort_values(by='Population')
groups = []
for group, subset in df.groupby(by='binnedPopulation'):
groups.append({
'BinRange': group,
'Count': len(subset),
'States': ','.join(subset.Abbreviation)
})
print(pd.DataFrame(groups))
# The `cut` method for _pandas_ data splits the dataset into bins. There are a number of arguments for the method. The following code creates equal sized bins. The method `value_counts` returns a frequency table.
binnedPopulation = pd.cut(state['Population'], 10) #cut into 10 bins
print(binnedPopulation.value_counts())
binnedPopulation.name = 'binnedPopulation'
df = pd.concat([state, binnedPopulation], axis=1)
df = df.sort_values(by='Population')
groups = []
for group, subset in df.groupby(by='binnedPopulation'):
groups.append({
'BinRange': group,
'Count': len(subset),
'States': ','.join(subset.Abbreviation)
})
print(pd.DataFrame(groups))
binnedPopulation.name = 'binnedPopulation'
df = pd.concat([state, binnedPopulation], axis=1)
df = df.sort_values(by='Population')
df
ax = (state['Population'] / 1_000_000).plot.hist(figsize=(4, 4))
ax.set_xlabel('Population (millions)')
plt.tight_layout()
plt.show()
plt.close("all")
plt.tight_layout()
plt.show()
ax = (state['Population'] / 1_000_000).plot.hist(figsize=(4, 4))
ax.set_xlabel('Population (millions)')
plt.tight_layout()
plt.show()
# Density is an alternative to histograms that can provide more insight into the distribution of the data points. Use the argument `bw_method` to control the smoothness of the density curve.
#pandas provides the density method
ax = state['Murder.Rate'].plot.hist(density=True, xlim=[0, 12],
bins=range(1,12),
figsize=(4, 4))
state['Murder.Rate'].plot.density(ax=ax)
ax.set_xlabel('Murder Rate (per 100,000)')
plt.tight_layout()
plt.show()
plt.close("all")
# Density is an alternative to histograms that can provide more insight into the distribution of the data points. Use the argument `bw_method` to control the smoothness of the density curve.
#pandas provides the density method
# plt.close("all")
ax = state['Murder.Rate'].plot.hist(density=True, xlim=[0, 12],
bins=range(1,12),
figsize=(4, 4))
state['Murder.Rate'].plot.density(ax=ax)
ax.set_xlabel('Murder Rate (per 100,000)')
plt.tight_layout()
plt.show()
state['Murder.Rate'].plot.hist(density=True, xlim=[0, 12],
bins=range(1,12),
figsize=(4, 4))
plt.close("all")
ax = state['Murder.Rate'].plot.hist(density=True, xlim=[0, 12],
plt.tight_layout()
plt.show()
ax.show()
# Density is an alternative to histograms that can provide more insight into the distribution of the data points. Use the argument `bw_method` to control the smoothness of the density curve.
#pandas provides the density method
plt.close("all")
ax = state['Murder.Rate'].plot.hist(density=True, xlim=[0, 12],
bins=range(1,12),
figsize=(4, 4))
state['Murder.Rate'].plot.density(ax=ax)
ax.set_xlabel('Murder Rate (per 100,000)')
plt.tight_layout()
plt.show()
knitr::opts_chunk$set(echo = TRUE)
PSDS_PATH <- file.path((getwd()))
dfw <- read.csv(file.path(PSDS_PATH, 'data', 'dfw_airline.csv'))
dfw
barplot(as.matrix(dfw) / 6, cex.axis=0.8, cex.names=0.7,
xlab='Cause of delay', ylab='Count')
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
telecom <- sp500_px[, sp500_sym[sp500_sym$sector == 'telecommunications_services', 'symbol']]
sp500_px <- read.csv(file.path(PSDS_PATH, 'data', 'sp500_data.csv.gz'), row.names=1)
sp500_sym <- read.csv(file.path(PSDS_PATH, 'data', 'sp500_sectors.csv'), stringsAsFactors = FALSE)
telecom <- sp500_px[, sp500_sym[sp500_sym$sector == 'telecommunications_services', 'symbol']]
telecom <- telecom[row.names(telecom) > '2012-07-01',]
telecom_cor <- cor(telecom)
telec
telecom <- sp500_px[, sp500_sym[sp500_sym$sector == 'telecommunications_services', 'symbol']]
telecom <- telecom[row.names(telecom) > '2012-07-01',]
telecom_cor <- cor(telecom)
telec
telecom_cor
sp500_sym
sp500_px
sp500_sym
telecom <- sp500_px[, sp500_sym[sp500_sym$sector == 'telecommunications_services', 'symbol']]
telecom
sp500_sym[sp500_sym$sector == 'telecommunications_services', 'symbol']
sp500_sym[sp500_sym$sector == 'telecommunications_services',]
sp500_sym[sp500_sym$sector == 'telecommunications_services', "symbol"]
telecom <- sp500_px[, sp500_sym[sp500_sym$sector == 'telecommunications_services', "symbol"]]
telecom
row.names(telecom)
row.names(telecom) > '2012-07-01',
telecom <- telecom[row.names(telecom) > '2012-07-01',]
telecom
telecom_cor <- cor(telecom)
telecom_cor
reticulate::repl_python()
# Next we focus on funds traded on major exchanges (sector == 'etf').
etfs <- sp500_px[row.names(sp500_px) > '2012-07-01',
sp500_sym[sp500_sym$sector == 'etf', 'symbol']]
corrplot(cor(etfs), method='ellipse')
?corrplot
??corrplot
corrplot::corrplot(cor(etfs), method='ellipse')
reticulate::repl_python()
# plot(telecom$T, telecom$VZ, xlab='T', ylab='VZ', cex=.8)
plot(telecom$T, telecom$VZ, xlab='ATT (T)', ylab='Verizon (VZ)')
telecom$T
# plot(telecom$T, telecom$VZ, xlab='T', ylab='VZ', cex=.8)
telecom
telecom
plot(telecom$T, telecom$VZ, xlab='ATT (T)', ylab='Verizon (VZ)')
abline(h=0, v=0, col='grey')
# plot(telecom$T, telecom$VZ, xlab='T', ylab='VZ', cex=.8)
telecom
plot(telecom$T, telecom$VZ, xlab='ATT (T)', ylab='Verizon (VZ)')
abline(h=0, v=0, col='grey')
dim(telecom)
reticulate::repl_python()
quit
kc_tax0 <- subset(kc_tax, TaxAssessedValue < 750000 &
SqFtTotLiving > 100 &
SqFtTotLiving < 3500)
nrow(kc_tax0)
PSDS_PATH <- file.path((getwd()))
kc_tax <- read.csv(file.path(PSDS_PATH, 'data', 'kc_tax.csv.gz'))
kc_tax0 <- subset(kc_tax, TaxAssessedValue < 750000 &
SqFtTotLiving > 100 &
SqFtTotLiving < 3500)
nrow(kc_tax0)
PSDS_PATH <- file.path((getwd()))
kc_tax <- read.csv(file.path(PSDS_PATH, 'data', 'kc_tax.csv.gz'))
kc_tax0 <- subset(kc_tax, TaxAssessedValue < 750000 &
SqFtTotLiving > 100 &
SqFtTotLiving < 3500)
nrow(kc_tax0)
kc_tax0 %>% head()
kc_tax0 |> head()
reticulate::repl_python()
quit
# If the number of data points gets large, scatter plots will no longer be meaningful. Here methods that visualize densities are more useful. The `stat_hexbin` method for is one powerful approach.
graph <- ggplot(kc_tax0, (aes(x=SqFtTotLiving, y=TaxAssessedValue))) +
stat_binhex(color='white') +
theme_bw() +
scale_fill_gradient(low='white', high='blue') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
graph
# If the number of data points gets large, scatter plots will no longer be meaningful. Here methods that visualize densities are more useful. The `stat_hexbin` method for is one powerful approach.
graph <- ggplot2::ggplot(kc_tax0, (aes(x=SqFtTotLiving, y=TaxAssessedValue))) +
stat_binhex(color='white') +
theme_bw() +
scale_fill_gradient(low='white', high='blue') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
kc_tax0
library(ggplot2)
kc_tax0 %>% ggplot(aes(aes(x=SqFtTotLiving, y=TaxAssessedValue))) + geom_hex()
library(tidyverse)
kc_tax0 %>% ggplot(aes(aes(x=SqFtTotLiving, y=TaxAssessedValue))) + geom_hex()
kc_tax0 %>% ggplot(aes(x=SqFtTotLiving, y=TaxAssessedValue))) + geom_hex()
kc_tax0 %>% ggplot(aes(x=SqFtTotLiving, y=TaxAssessedValue)) + geom_hex()
# If the number of data points gets large, scatter plots will no longer be meaningful. Here methods that visualize densities are more useful. The `stat_hexbin` method for is one powerful approach.
graph <- ggplot2::ggplot(kc_tax0, (aes(x=SqFtTotLiving, y=TaxAssessedValue))) +
stat_binhex(color='white') +
theme_bw() +
scale_fill_gradient(low='white', high='blue') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
graph
kc_tax0 %>% ggplot(aes(x=SqFtTotLiving, y=TaxAssessedValue)) + geom_hex()
kc_tax0 %>% ggplot(aes(x=SqFtTotLiving, y=TaxAssessedValue)) + geom_hex(bins=10)
library(tidyverse)
kc_tax0 %>% ggplot(aes(x=SqFtTotLiving, y=TaxAssessedValue)) + geom_hex(bins=10) +
theme_bw() +
scale_fill_gradient(low='white', high='blue') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
kc_tax0
d <- ggplot(diamonds, aes(carat, price))
d + geom_hex()
install.packages("hexbin")
d <- ggplot(diamonds, aes(carat, price))
d + geom_hex()
library(tidyverse)
kc_tax0 %>% ggplot(aes(x=SqFtTotLiving, y=TaxAssessedValue)) + geom_hex() +
theme_bw() +
scale_fill_gradient(low='white', high='blue') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
library(tidyverse)
kc_tax0 %>% ggplot(aes(x=SqFtTotLiving, y=TaxAssessedValue)) + geom_hex(bins = 30) +
theme_bw() +
scale_fill_gradient(low='white', high='blue') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
library(tidyverse)
kc_tax0 %>% ggplot(aes(x=SqFtTotLiving, y=TaxAssessedValue)) + geom_hex(bins = 30, color = "white") +
theme_bw() +
scale_fill_gradient(low='white', high='blue') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
# If the number of data points gets large, scatter plots will no longer be meaningful. Here methods that visualize densities are more useful. The `stat_hexbin` method for is one powerful approach.
graph <- ggplot2::ggplot(kc_tax0, (aes(x=SqFtTotLiving, y=TaxAssessedValue))) +
stat_binhex(color='white') +
theme_bw() +
scale_fill_gradient(low='white', high='blue') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
graph
library(tidyverse)
kc_tax0 %>% ggplot(aes(x=SqFtTotLiving, y=TaxAssessedValue)) + geom_hex(color = "white") +
theme_bw() +
scale_fill_gradient(low='white', high='blue') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
reticulate::repl_python()
# Visualize as a two-dimensional extension of the density plot.
graph <- ggplot(kc_tax0, aes(SqFtTotLiving, TaxAssessedValue)) +
theme_bw() +
geom_point(color='blue', alpha=0.1) +
geom_density2d(color='white') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
graph
# Visualize as a two-dimensional extension of the density plot.
graph <- ggplot(kc_tax0, aes(SqFtTotLiving, TaxAssessedValue)) +
theme_bw() +
geom_density2d(color='white') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
graph
# Visualize as a two-dimensional extension of the density plot.
graph <- ggplot(kc_tax0, aes(SqFtTotLiving, TaxAssessedValue)) +
theme_bw() +
geom_point(color='blue', alpha=0.1) +
geom_density2d(color='white') +
scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +
labs(x='Finished Square Feet', y='Tax-Assessed Value')
graph
reticulate::repl_python()
reticulate::repl_python()
